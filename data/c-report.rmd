---
title: "Some Internet Data"
output:
  html_document:
    code_folding: hide
    df_print: paged
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float: yes
  word_document:
    toc: yes
    toc_depth: '6'
---


# Introduction


Through this project, we seek to understand the relationship between internet access and inequality on a global scale before examining the factors in detail using American Census Data. We take this dual-pronged approach so that we can get a picture of Internet Inequality globally while using reliable US Census data to do the quantitaive analysis. We will use data from the United Nations to compare selected socio-economic indexes to international communication measurements.

## Questions

> What economic indicators (race, occupation, community poverty rate) are most strongly correlated with internet access rates? Can we build a model that accurately predicts said rates? 

> Are internet access rates a stronger predictor of poverty rates than other forms of social investment (ie roads, schools, hospitals)?

> Do these effects extend across internet technologies (cell phones and broadband internet)? If not, which type of infrastructure investment is better.


## Motivation

We are interested in this problem as data scientists because our field is a mixed bag. On one hand, big data can be used to influence elections, spread hateful propaganda, and be used to track every purchase and decision we make. These political consequences are well known. However, the Internet has a history of advancing economies, and those without the internet [tend to be left behind](https://www.pewresearch.org/fact-tank/2015/04/30/racial-and-ethnic-differences-in-how-people-use-mobile-technology/). To speak about this in particular, we need to investigate the ways in which internet access influences occupational outlook while controlling for other confounding factors like geography, race, and infrastructure investment more generally. 

## Literature Review

## Data 
[American Community Survey](https://www.census.gov/data/developers/data-sets/acs-1year.html)    

[Annual Survey of State Finances](https://www.census.gov/programs-surveys/state.html)  

[World Bank Data](https://data.worldbank.org/indicator)  

[IEE MAC Address Blocks](http://standards-oui.ieee.org/oui.txt)  

[List of Internet Exchange Points](https://en.wikipedia.org/wiki/List_of_Internet_exchange_points#Active_internet_exchanges)



## Methodology

First we will examine the problem on a global scale using chloropleth maps that will inform our future choices.

We will build several models for predicting poverty rate, using both the generalized logistic model and the generalized linear model. In this way, we'll see how things like internet access and infrastructure investment influence poverty rates. The American Community Survey includes internet access rates, poverty, race, industry, language, occupation, place of birth, and familial origin. Using this data alone, we should be able to see if race or occupation is a better indicator of aggregate povery than internet access rates.




## Hypothesis

Pew Research says that [20% of teens are unable to finish their homework](https://www.pewresearch.org/fact-tank/2018/10/26/nearly-one-in-five-teens-cant-always-finish-their-homework-because-of-the-digital-divide/) due to the digital divide. The end result of this is likely low-skill careers and lower incomes. In fact, the internet tends to raise the tide for all, as a breadth study (also by Pew) showed that [per capita income and access rates are highly correlated](https://www.pewglobal.org/2016/02/22/internet-access-growing-worldwide-but-remains-higher-in-advanced-economies/technology-report-02-06c/). We'd like to investigate the relationship between technology and the economy and see if we can build models resilient to the particle type of device. [Previous work](https://www.mdpi.com/2072-4292/11/4/375) has used infrastructure invesment to build logistic models for poverty using satellite images of infrastructure. It is also well known that poverty and broadband access rates are [highly correlated](http://overflow.solutions/demographic-data/how-poverty-status-in-each-u-s-county-relates-to-internet-access/). However, it is unknown if there is an underlying causal factor or if internet can, _by itself_, lift people out of poverty. The McKinsey Global Institute did a massive study on the economic potential of [internet investment in China](https://www.mckinsey.com/~/media/McKinsey/Industries/High%20Tech/Our%20Insights/Chinas%20digital%20transformation/MGI%20China%20digital%20Full%20report.ashx) that will inform our approach in this matter. Finally, the Internet Society, a global organization that builds internet infrastructure (mostly in the developing world), has compiled [a list of internet penetration rates and other such metrics](https://www.internetsociety.org/wp-content/uploads/2017/08/Global_Internet_Report_2014_0.pdf) by country across the world. However, due to data collection limitations and the quality of data sources across continents, it would be impossible to investigate these things wtih respect to more generic features like race and infrastructure. Since the United States has a non-uniform income distribution across states, this should allow us to draw from a breadth of circumstances. Due to the multiplicative of effects in education, business opportunities, and spending opportunities available on the Internet, we suspect that governmental investment in digital infrastructure will have at least as much affect as road or school spending. Additionally, we suspect that this multiplier is reduced for cellular infrastructure relative to fixed (broadband) infrastructure because of the productivity gains associated with PCs over smartphones. This research will reveal to governments (both local and national) what kinds of infrastructure investment yields the most economic gains in the digital age. To our knowledge, this particular question has not been answered.

# Executive Summary


TODO


```{r libs, eval=T, echo=F, warning=F, message=F}
library(caret)
library(caTools)
library(corrplot)
library(e1071)
library(fastDummies)
library(forecast)
library(ggplot2)
library(imputeTS)
library(lattice)
library(knitr)
library(ModelMetrics)
library(nnet)
library(randomForest)
library(readxl)
library(reshape2)
library(tidyr)
library(tidyverse)
library(xlsx)
library(rworldmap)
# Show loaded packages.
(.packages())
```
# Internet Inequality-- A Global Persepective

```{r map1, eval=T, echo=T, warning=F, message=F}
df <- read.csv("Global/InternetIndicators.csv")

map <- suppressWarnings( joinCountryData2Map(df, nameJoinColumn = "Country", joinCode = "ISO3"))

df$IXP[is.na(df$IXP)] <- 0

mapCountryData(map, nameColumnToPlot = "Broadband", mapTitle = "Broadband Subcriptions per 100 people", catMethod = 'FixedWidth', colourPalette = "diverging", missingCountryCol = "grey")
```

Immediately, we can see that broadband subscription rates are higher in strongly developed places like North America, Western Europe, and Australia. Conversely, poor countries across South America, Africa, and South Asia have significantly lower broadband access rates. Please note that countries in grey have unknown values.

```{r map2, eval=T, echo=T, warning=F, message=F}
mapCountryData(map, nameColumnToPlot = "Cells", mapTitle = "Cell Phone Subscriptions per 100 People", catMethod = 'FixedWidth',colourPalette = "diverging" , missingCountryCol = "grey")

```


However, the number of cell phone subscriptions per 100 people is much more uniform. That is due to the lower cost of wireless network deployment compared to the capital-intesive processes of digging trenches to lay copper or fiber.

```{r map3, eval=T, echo=T, warning=F, message=F}
mapCountryData(map, nameColumnToPlot = "Servers", mapTitle = "Servers per 10,000 people", catMethod = 'FixedWidth',colourPalette = "diverging" , missingCountryCol = "grey")

```

When we look at the number of servers available in each country, we find that Western Europe has the highest per capita server load. Countries like New Zealand and South Africa are also high because they are conveniently located for undersea cables that compose the back-bone of the internet. TODO Source

```{r map4, eval=T, echo=T, warning=F, message=F}
mapCountryData(map, nameColumnToPlot = "mac.addresses", mapTitle = "Mac Addresses Blocks Assigned per Country", catMethod = 'FixedWidth',colourPalette = "diverging" , missingCountryCol = "grey")
```

The IEEE is a global organization that manages technological standards, publishes and circulates literature about electronics and and electrical engineering. In addition, they allocate MAC addresses which are the physical address of every bluetooth/wifi radio, ethernet port, and fiber cable on the internet. As we can see, a relatively small number of countries have original electronics manufacturers, with the US registering more than twice the number of devices as the next country (China). 

```{r map5, eval=T, echo=T, warning=F, message=F}
mapCountryData(map, nameColumnToPlot = "IXP", mapTitle = "Internet Exchange Points by Country", catMethod = 'FixedWidth',colourPalette = "diverging" , missingCountryCol = "grey")
```

The undersea cables mentioned earlier wind up at one of 600 buildings around the world where network operators connect their computers to their peers and create what we think of as the 'inter' net. These 600 buildings are not even distributed, with most countries only have a single access point to the Internet. In addition, regimes known for censorship (ie Egypt, Turkey, and China) have relatively few internet exchange points, allowing for centralized control and censorship. TODO: Source

```{r map6, eval=T, echo=T, warning=F, message=F}
mapCountryData(map, nameColumnToPlot = "Exports", mapTitle = "High Tech Exports (2017 USD)", catMethod = 'fixedWidth', colourPalette = "diverging" , missingCountryCol = "grey")


```

The net result of the modern Internet infrastructure is a centralized model with a few players making all of the profits. Above we see the total amount of high tech exports as measured in 2017 USD. Three countries account for the bulk of the profit here, seeming to indicate that a centralized Internet infrastructure does not raise the standards for everybody. It is apparent that today's paradigm encourages consumption over creation.

# Correlation between Various Technology Indicators and Poverty Rates

## Data Initialization and Preprocessing




```{r init1, eval=T, echo=T, warning=F, message=F}
training <- read_csv(file = "Cleaned_Community_Data.csv")
#test     <- read_excel("StudentEvaluation.xlsx")


#test <- dummy_cols(test, select_columns = test$Brand.Code)training <- as.data.frame(sapply(training, as.numeric))
#test     <- as.data.frame(sapply(test, as.numeric))


#names(test) <- make.names(names(test))

#training$Brand.Code <- NULL
#test$Brand.Code <- NULL
states <- training$State
training$State <- NULL
training$X1 <- NULL
head(training)

```


### Fixing Missing Data

Below we impute the missing values using a monotone cubic approximator (known as a Stineman interpolation). It has a tendency to perform well on linear as well as higher-order data vectors.

```{r impute1, eval=T, echo=T, warning=F, message=F}
training <- na.interpolation(training)
f=function(x){
   x<-as.numeric(as.character(x)) #first convert each column into numeric if it is from factor
   x[is.na(x)] =median(x, na.rm=TRUE) #convert the item with NA to median value from the column
   x #display the column
}
training <- data.frame(sapply(training, f))
training
#test <- na.interpolation(test, option = 'stine')
```

Below we can see the mean and standard deviation for each data vector. As we can see, our data occurs across many orders of magnitude. For the best fit, the data should be centered and scaled. 

```{r mean_sd, eval=T, echo=T, warning=F, message=F}
means <- sapply(training, mean, na.rm = TRUE)
sds   <- sapply(training, sd, na.rm = TRUE)
explore <- as.data.frame(cbind( means, sds))
ggplot(explore, aes(x = row.names(explore), y = means))+ 
  geom_bar(stat = 'identity') + 
  labs(title = "Means of Various Features") + 
  xlab("Data Features") + 
  ylab("Mean of Data") +
  theme(panel.background = element_blank()) + 
  geom_errorbar(aes(ymin = means - sds, ymax = means + sds))
```

These plots confirm the non-normality of most of our data. For the non-linear models, we must center and scale them. Additionally, we will need other data transformations (discussed below).
TODO: Density Plots
```{r figure1, fig.height=50}
#ggplot(data = gather(training), mapping = aes(x = value)) + 
#  geom_histogram(aes(y=..density..,), colour="black", fill="white")+
#  geom_density(alpha=.2, fill="lightgrey")+
#  facet_wrap(~key, ncol = 1, scales = 'free') 
```


## Correlation Plot of Predictors

We can see significant covariance in the data. Additionally, many data points have near-zero variance. Excluding these confounding variables will improve our model.

```{r corrplot, eval=T, echo=T, warning=F, message=F, fig.height=50}
results <- cor(training, method = 'pearson')
corrplot::corrplot(results, method = 'circle')
```



## Data Pre-processing

For preprocessing of data, we remove near zero predictors, fill in missing values with KNN method, and transform predictors using the Yeo-Johnson transformation method. We also center and scale the data. Additionally, we remove the covariant terms and the ones with near zero variance here, since they will not improve our models.

```{r pre_proc, eval=T, echo=T, warning=F, message=F}
x_train <- subset(training, select = -Percent.Below.Poverty.Line )
y_train <- training$Percent.Below.Poverty.Line

preProcValues <- preProcess(x_train, method = c("center", "scale", "YeoJohnson", "nzv", "corr"))
trainTransformed <- predict(preProcValues, x_train)
```



## Support Vector Machine

Comparing SVM with Neural Networks (NN), both are non-linear algorithms. A Support Vector Machine with different kernels is comparable to a Neural Network with different layers. One advantage SVMs have over NNs is that NNs need large amounts of data to train, SVMs work with smaller-sized data with less computing power. Finally SVM usually only have 2-3 parameters to tune, they are easy to code, and the results are explainable.
On the other hand, SVMs might not beat NNs on the accuracy metric.


```{r svm1, eval=T, echo=T, warning=F, message=F}
set.seed(100)

y_train.orig = y_train
sample = sample.split(y_train.orig, SplitRatio = .75)
y_train_svm = subset(y_train.orig, sample == TRUE)
y_test_svm = subset(y_train.orig, sample == FALSE)

training_svm = subset(trainTransformed, sample == TRUE)
test_svm = subset(trainTransformed, sample == FALSE)

fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 3,
                           ## repeated ten times
                           repeats = 3)
svmFit <- train(training_svm, y_train_svm,
                 method = "svmRadial",
                 trControl = fitControl,
                 tuneLength = 8,
                 metric = "RMSE")
svmFit
model1 <- svmFit

rmse.svm.train = rmse(predict(svmFit, training_svm), y_train_svm)
r2.svm.train = R2(predict(svmFit, training_svm), y_train_svm)
rmse.svm.test = rmse(predict(svmFit, test_svm), y_test_svm)
r2.svm.test = R2(predict(svmFit, test_svm), y_test_svm)


kable(data.frame(Model=c("Support Vector Machine"), RMSE.train=min(rmse.svm.train), RSquared.train=min(r2.svm.train), RMSE.test=min(rmse.svm.test), RSquared.test=min(r2.svm.test)))
```






## Random Forest Model

Random forests are a modification of bagging that builds a large collection of de-correlated
trees [1]. They are considered to belong in the category of non-parametric models since 
the number of parameters grows with the size of the training set. They are considered to be
an improvement to the use of CART (Classification and Regression Tree) models because they
do not suffer from some of the problems associated with CART models, such as the fact that
CART models are unstable: small changes to the structure of the input data can have large
effects on the CART model [2]. Random forests are designed to be low-variance estimators.

Random forests are based on the basic idea of aggregating uncorrelated sets of predictors,
since one way to reduce the variance of an estimate is to average several estimates together [2].
A random forest trains a randomly chosen set of input variables over a randomly chosen subset of
the data, and aggregates together several such trees to produce an overall estimator. Random
forests have proven to be quite successful in a variety of real-world applications and often
are seen to generalize very well to unseen real-world data.


```{r RF1, eval=T, echo=T, warning=F, message=F}

# Split the training data into a portion that is withheld from the model and used to evaluate
# the model.

set.seed(123)
sample = sample.split(training$Percent.Below.Poverty.Line, SplitRatio = .75)
training_forest = subset(training, sample == TRUE)
training_forest
test_forest = subset(training, sample == FALSE)

set.seed(123)
rfFit <- randomForest::randomForest(Percent.Below.Poverty.Line ~ ., data = training_forest, importance = TRUE,
                                    ntree = 100, keep.forest = TRUE)
model2 <- rfFit

varImpPlot(rfFit, n.var=10,
           main="Important Variables in Random Forest Model (top 10 shown)")
```


We now compute RMSE values for the Random Forest model on both the training and the test (withheld) portion of the data set.

```{r RF_rmse, eval=T, echo=T, warning=F, message=F}
training_forest2 = dplyr::select(training_forest, -Percent.Below.Poverty.Line)
rfPred.train = predict(rfFit, training_forest2)
rmse.rf.train = rmse(training_forest$Percent.Below.Poverty.Line, rfPred.train)
r2.rf.train = R2(training_forest$Percent.Below.Poverty.Line, rfPred.train)
test_forest2 = dplyr::select(test_forest, -Percent.Below.Poverty.Line)
rfPred.test = predict(rfFit, test_forest2)
rmse.rf.test = rmse(test_forest$Percent.Below.Poverty.Line, rfPred.test)
r2.rf.test = R2(test_forest$Percent.Below.Poverty.Line, rfPred.test)
kable(data.frame(Model=c("Random Forest"), RMSE.train=c(rmse.rf.train), RSquared.train=c(r2.rf.train), RMSE.test=c(rmse.rf.test), RSquared.test=c(r2.rf.test)))
```


## Neural Network Model

Neural Networks are a powerful nonlinear technique inspired by theories about how the human brain works [5]. Neural Networks can be classifiers (when the output variable is categorical) or regression (when the output variable is numeric). In this problem we use a regression artificial neural network (ANN) using the nnet package in R. Below we build and evaluate a Neural Network model of the regression problem.


```{r nnet1, eval=T, echo=T, warning=F, message=F}
set.seed(123)
training_nn = training_forest2
training_nn_y = training_forest$Percent.Below.Poverty.Line
test_nn = test_forest2
test_nn_y = test_forest$Percent.Below.Poverty.Line
  
nnetFit <- nnet(training_nn, training_nn_y,
                size = 4,
                decay = 0.01,
                linout = TRUE,
                trace = FALSE,
                maxit = 500, # Iterations
                ## Number of parameters used by the model
                MaxNWts= 4 * (ncol(training_nn) + 1) + 5 + 1)
model3 <- nnetFit
nnetFit
rmse.nnet.train = rmse(training_nn_y, predict(nnetFit, training_nn))
r2.nnet.train = mean(cor(training_nn_y, training_nn)^2)
rmse.nnet.test = rmse(test_nn_y, predict(nnetFit, test_nn))
r2.nnet.test = mean(cor(test_nn_y, test_nn)^2)
kable(data.frame(Model=c("NeuralNet"), RMSE.train=c(rmse.nnet.train), RSquared.train=c(r2.nnet.train), RMSE.test=c(rmse.nnet.test), RSquared.test=c(r2.nnet.test)))



```



## Generalized Linear Model

Because our output value is continuous and our data is numeric, we can use a generalized linear model to compute the pH. These models are generic and assume linearity in response. We will use the "Gaussian" type which assumes normally distributed variables

```{r glm1, eval=T, echo=T, warning=F, message=F}
set.seed(123)
training_glm = subset(training, sample == TRUE)
test_glm = subset(training, sample == FALSE)

glm_train_label <- training_glm$Percent.Below.Poverty.Line
glm_test_label <- test_glm$Percent.Below.Poverty.Line
glm_train <- select(training_glm, -Percent.Below.Poverty.Line)
glm_test <- select(test_glm, -Percent.Below.Poverty.Line)

model4 <- glm(glm_train_label ~., glm_train, family = "gaussian")
model5 <- step(model4, direction = "backward", trace = FALSE)

sum4 <- summary(model4)
sum5 <- summary(model5)


plot(model5)


rmse.glm.train <- rmse(predict(model5, glm_train), glm_train_label)
r2.glm.train <- R2(predict(model5, glm_train), glm_train_label)
rmse.glm.test <- rmse(predict(model5, glm_test), glm_test_label)
r2.glm.test <- R2(predict(model5, glm_test), glm_test_label)
kable(data.frame(Model=c("GLM"), RMSE.train=c(rmse.glm.train), RSquared.train=c(r2.glm.train), RMSE.test=c(rmse.glm.test), RSquared.test=c(r2.glm.test)))
```

## Conclusion: Social Indicators

Since we see that the Random Forest model produced the lowest RMSE on the withheld training data, we select it as the best model to predict poverty rates.


```{r compare_and_predict, eval=T, echo=T, warning=F, message=F}
modelperf = data.frame(matrix(ncol=4, nrow=8))
colnames(modelperf) = c("Dataset", "Model", "RMSE", "RSquared")


modelperf[1,] = list("Train", "Random Forest", rmse.rf.train, r2.rf.train)
modelperf[2,] = list("Test", "Random Forest", rmse.rf.test, r2.rf.test)
modelperf[3,] = list("Train", "SVM", rmse.svm.train, r2.svm.train)
modelperf[4,] = list("Test", "SVM", rmse.svm.test, r2.svm.test)
modelperf[5,] = list("Train", "GLM", rmse.glm.train, r2.glm.train)
modelperf[6,] = list("Test", "GLM", rmse.glm.test, r2.glm.test)
modelperf[8,] = list("Train", "Nnet", rmse.nnet.test, r2.nnet.train)
modelperf[7,] = list("Test", "NNet", rmse.nnet.train, r2.nnet.test)

ggplot(data=modelperf, aes(x=reorder(Model, RMSE), y=RMSE, fill=Dataset)) +
    geom_bar(stat="identity", position=position_dodge()) +
    ggtitle("Model RMSE for Percent.Below.Poverty.Line Prediction")


ggplot(data=modelperf, aes(x=reorder(Model, -RSquared), y=round(RSquared,3), fill=Dataset)) +
    geom_bar(stat="identity", position=position_dodge()) +
    ggtitle("Model RSquared for Percent.Below.Poverty.Line Prediction")

bestFit = model2


```

# Finance Analysis

```{r}
finances <- read.csv("Cleaned_Finance_Data.csv")

target <- finances$Poverty

finances$X <- NULL
finances <- as.data.frame(cbind(finances$Education,
finances$Hospitals,
finances$Health,
finances$Highways,
finances$Parks.and.recreation,
finances$Police.protection,
finances$Governmental.administration,
finances$Correction,
finances$Natural.resources,
finances$Internet,
finances$Smartphone,
finances$Computer,
finances$Public.welfare,
finances$Debt.at.end.of.fiscal.year))


preProcValues <- preProcess(finances, method = c("center", "scale", "YeoJohnson", "nzv", "corr"))
finances <- predict(preProcValues, finances)
finances

colnames(finances) <- c("Education", "Hospitals", "Health", "Highways", "Parks", "Police",  "Administration", "Prisons", "Internet", "Smartphone", "Computer", "Public.Welfare", "Debt")

finances$Poverty <- target

finances
```


```{r RF2, eval=T, echo=T, warning=F, message=F}

# Split the training data into a portion that is withheld from the model and used to evaluate
# the model.

set.seed(123)
set.seed(123)

sample = sample.split(finances$Poverty, SplitRatio = .75)
training_forest = subset(finances, sample == TRUE)
test_forest = subset(finances, sample == FALSE)
rfFit <- randomForest::randomForest(Poverty ~ ., data = training_forest, importance = TRUE,
                                    ntree = 100, keep.forest = TRUE)
model6 <- rfFit

varImpPlot(rfFit, n.var=10,
           main="Important Variables in Random Forest Model (top 10 shown)")
as.data.frame(training_forest)
training_forest2 = dplyr::select(training_forest, -Poverty)
rfPred.train = predict(rfFit, training_forest2)
rmse.rf.train = rmse(training_forest$Poverty, rfPred.train)
r2.rf.train = R2(training_forest$Poverty, rfPred.train)
test_forest2 = dplyr::select(test_forest, -Poverty)
rfPred.test = predict(rfFit, test_forest2)
rmse.rf.test = rmse(test_forest$Poverty, rfPred.test)
r2.rf.test = R2(test_forest$Poverty, rfPred.test)
kable(data.frame(Model=c("Random Forest"), RMSE.train=c(rmse.rf.train), RSquared.train=c(r2.rf.train), RMSE.test=c(rmse.rf.test), RSquared.test=c(r2.rf.test)))

```

```{r glm2, eval=T, echo=T, warning=F, message=F}
set.seed(123)
training_glm = subset(finances, sample == TRUE)
test_glm = subset(finances, sample == FALSE)

glm_train_label <- training_glm$Poverty
glm_test_label <- test_glm$Poverty
glm_train <- select(training_glm, -Poverty)
glm_test <- select(test_glm, -Poverty)

model4 <- glm(glm_train_label ~., glm_train, family = "gaussian")
model5 <- step(model4, direction = "backward", trace = FALSE)

sum4 <- summary(model4)
sum5 <- summary(model5)


rmse.glm.train <- rmse(predict(model5, glm_train), glm_train_label)
r2.glm.train <- R2(predict(model5, glm_train), glm_train_label)
rmse.glm.test <- rmse(predict(model5, glm_test), glm_test_label)
r2.glm.test <- R2(predict(model5, glm_test), glm_test_label)
kable(data.frame(Model=c("GLM"), RMSE.train=c(rmse.glm.train), RSquared.train=c(r2.glm.train), RMSE.test=c(rmse.glm.test), RSquared.test=c(r2.glm.test)))
```

# Next Steps

There are many ways we can continue improving the model performance, one method could be running more times of cross validation on more folds than 3 times 3-fold we have now for SVM and GBM models. It would take a longer time to compute, but the results would likely be better. Finally, more data would help us build a better model, in particular because the gap between the test and train sets tends to be relatively large across all of the models, with the exception of the Random Forest regressor because it builds its model over 1000 iterations.

We could also use time-series data to examine causal relationships between the data.


# References
TO DO: APA format
1. Random Forests. https://uc-r.github.io/random_forests
2. Kevin Murphy (2012). Machine Learning a Probabilistic Perspective.
3. Support Vector Machine. https://uc-r.github.io/svm
4. Support Vector Machines. http://web.mit.edu/6.034/wwwbob/svm.pdf
5. Kuhn et al (2013). Applied Predictive Modeling
